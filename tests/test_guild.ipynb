{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from macronet.flowfunc import FuncModel as fn\n",
    "import MoNet.monet as mn\n",
    "import importlib\n",
    "importlib.reload(mn)\n",
    "# importlib.reload(fn)\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "monet 支持的接口：\n",
    "\n",
    "- `Fn(args=[],call:str | Callable ='SEQ')`\n",
    "- `X = mn.Fn()`\n",
    "- `Adup(module,i=0,o=0,net='',mn_dict={},in_dim=-1)`\n",
    "  - mn.Fn()\n",
    "- `Layer(i=0,o_lists=[10,[32,32],1],net_lists=['dp_0.5',[\"fc\",'bn','act','dp_0.5'],\"fc\"],mn_dict=mn_dict,in_dim=-1)`\n",
    "  - Layer(i=0, o_lists=1, net_lists=\"fc_1\") # 输入维度，输出维度，网络名称模式\n",
    "  - Layer(\"fc_1\", 1) # 省略输入维度的自适应模式\n",
    "- `Fn().p()`\n",
    "\n",
    "monet 支持的计算：\n",
    "\n",
    "- \\+\n",
    "- \\&\n",
    "- \\*\n",
    "- \\*\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1] ['fc', 'act']\n",
      "SEQ>Fn(\n",
      "  (0:fc): @Adup:Linear(in_features=0, out_features=1, bias=True) *id:1798851407456\n",
      "  (1:act): @Adup:GELU(approximate='none') *id:1798851409040\n",
      ")\n",
      "[Warning] Input dim is not match, set to 10\n",
      "[1] ['fc']\n",
      "tensor([[-0.1489]], grad_fn=<GeluBackward0>)\n",
      "SEQ>Fn(\n",
      "  (0:fc): @Adup:Linear(in_features=10, out_features=1, bias=True) *id:1798851407456\n",
      "  (1:act): @Adup:GELU(approximate='none') *id:1798851409040\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(1,10)\n",
    "# F = mn.Fn()*mn.X\n",
    "# print(F);print(F(x));print(F)# 空的网络会直接返回输入的x\n",
    "# F = mn.Layer(0,1,'fc')\n",
    "# print(F);print(F(x));print(F) # 设置为0时会自动调整输入输出维度\n",
    "F = mn.Layer(0,1,['fc','act']) # 输入 输出 网络\n",
    "print(F);print(F(x));print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 20], 30] [['fc', 'bn'], 'act']\n",
      "[10, 20] [['fc', 'bn'], ['fc', 'bn']]\n",
      "[10, 10] ['fc', 'bn']\n",
      "[20, 20] ['fc', 'bn']\n",
      "SEQ>Fn(\n",
      "  (0:mix): SEQ>Fn(\n",
      "    (cell-0): SEQ>Fn(\n",
      "      (0:fc): @Adup:Linear(in_features=0, out_features=10, bias=True) *id:1798851409184\n",
      "      (1:bn): @Adup:BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) *id:1798851409328\n",
      "    )\n",
      "    (cell-1): SEQ>Fn(\n",
      "      (0:fc): @Adup:Linear(in_features=10, out_features=20, bias=True) *id:1798851408800\n",
      "      (1:bn): @Adup:BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) *id:1798851410864\n",
      "    )\n",
      "  )\n",
      "  (1:act): @Adup:GELU(approximate='none') *id:1798851409424\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# # 这四种等价，都是一个对一个\n",
    "# print(mn.Layer('fc',10))\n",
    "# print(mn.Layer('fc',[10]))\n",
    "# print(mn.Layer(['fc'],10))\n",
    "# print(mn.Layer(['fc'],[10]))\n",
    "\n",
    "# # 第一维度，一一对应，缺少部分重复最后一个到一样长\n",
    "# print(mn.Layer('fc',[10,20]))\n",
    "# print(mn.Layer(['fc'],[10,20]))\n",
    "# print(mn.Layer(['fc','act'],[10,20]))\n",
    "# print(mn.Layer(['fc','act'],10))\n",
    "# print(mn.Layer(['fc','act'],[10]))\n",
    "\n",
    "# 在第二维度，网络代表当成一个整体，而输出代表重复\n",
    "# print(mn.Layer(['fc','act'],[[10,20],30]))\n",
    "# print(mn.Layer([['fc','bn'],'act'],[10,20]))\n",
    "print(mn.Layer([['fc','bn'],'act'],[[10,20],30]))\n",
    "\n",
    "# 再复杂的网络采用*或者+连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, [0, 10], 1] ['dp_0.5', ['fc', 'act'], 'fc']\n",
      "[0, 10] [['fc', 'act'], ['fc', 'act']]\n",
      "[0, 0] ['fc', 'act']\n",
      "[10, 10] ['fc', 'act']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SEQ>Fn(\n",
       "  (0): @Adup:Dropout(p=0.5, inplace=False) *id:1798851410864\n",
       "  (1): SEQ>Fn(\n",
       "    (cell-0): SEQ>Fn(\n",
       "      (0:fc): @Adup:Linear(in_features=10, out_features=0, bias=True) *id:1798851405392\n",
       "      (1:act): @Adup:GELU(approximate='none') *id:1798851410624\n",
       "    )\n",
       "    (cell-1): SEQ>Fn(\n",
       "      (0:fc): @Adup:Linear(in_features=0, out_features=10, bias=True) *id:1798851409712\n",
       "      (1:act): @Adup:GELU(approximate='none') *id:1798851407024\n",
       "    )\n",
       "  )\n",
       "  (2): @Adup:Linear(in_features=10, out_features=1, bias=True) *id:1798851410384\n",
       "  (3): LIC>Fn(\n",
       "    (0): @dup:max *id:1798851407120\n",
       "    (1): @dup:min *id:1798851406976\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F=mn.Layer(0,[10,[0,10],1],['dp_0.5',['fc','act'],'fc'])\n",
    "F1=F*(fn(max)+min)\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ SEQ>Fn( >>\n",
      "    tensor([[ 1.1471, -2.2010, -0.5649,  0.6266,  1.3925,  1.4525,  0.0057, -1.0800,\n",
      "         -0.2058,  0.2350],\n",
      "        [-0.7154,  0.0075,  0.2986,  0.8201,  0.3921,  0.0758, -0.1757, -0.0256,\n",
      "          0.8291,  0.2075]])\n",
      "[Warning] Input dim is not match, set to 10\n",
      "[10] ['dp_0.5']\n",
      "@ @Adup:Dropout(p=0.5, inplace=False) *id:1798851410864 >>\n",
      "    tensor([[ 1.1471, -2.2010, -0.5649,  0.6266,  1.3925,  1.4525,  0.0057, -1.0800,\n",
      "         -0.2058,  0.2350],\n",
      "        [-0.7154,  0.0075,  0.2986,  0.8201,  0.3921,  0.0758, -0.1757, -0.0256,\n",
      "          0.8291,  0.2075]])\n",
      "  Net : Dropout(p=0.5, inplace=False)\n",
      "  $ (tensor([[ 1.1471, -2.2010, -0.5649,  0.6266,  1.3925,  1.4525,  0.0057, -1.0800,\n",
      "         -0.2058,  0.2350],\n",
      "        [-0.7154,  0.0075,  0.2986,  0.8201,  0.3921,  0.0758, -0.1757, -0.0256,\n",
      "          0.8291,  0.2075]]),) >>\n",
      "  == tensor([[ 2.2941, -4.4021, -0.0000,  1.2533,  0.0000,  2.9049,  0.0000, -0.0000,\n",
      "         -0.4117,  0.4699],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.7841,  0.1516, -0.3514, -0.0000,\n",
      "          1.6582,  0.4151]])\n",
      ")>> tensor([[ 2.2941, -4.4021, -0.0000,  1.2533,  0.0000,  2.9049,  0.0000, -0.0000,\n",
      "         -0.4117,  0.4699],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.7841,  0.1516, -0.3514, -0.0000,\n",
      "          1.6582,  0.4151]])\n",
      "  0:dp : @Adup:Dropout(p=0.5, inplace=False) *id:1798851410864\n",
      "  $ (tensor([[ 1.1471, -2.2010, -0.5649,  0.6266,  1.3925,  1.4525,  0.0057, -1.0800,\n",
      "         -0.2058,  0.2350],\n",
      "        [-0.7154,  0.0075,  0.2986,  0.8201,  0.3921,  0.0758, -0.1757, -0.0256,\n",
      "          0.8291,  0.2075]]),) >>\n",
      "  == tensor([[ 2.2941, -4.4021, -0.0000,  1.2533,  0.0000,  2.9049,  0.0000, -0.0000,\n",
      "         -0.4117,  0.4699],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.7841,  0.1516, -0.3514, -0.0000,\n",
      "          1.6582,  0.4151]])\n",
      "@ SEQ>Fn( >>\n",
      "    tensor([[ 2.2941, -4.4021, -0.0000,  1.2533,  0.0000,  2.9049,  0.0000, -0.0000,\n",
      "         -0.4117,  0.4699],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.7841,  0.1516, -0.3514, -0.0000,\n",
      "          1.6582,  0.4151]])\n",
      "@ SEQ>Fn( >>\n",
      "    tensor([[ 2.2941, -4.4021, -0.0000,  1.2533,  0.0000,  2.9049,  0.0000, -0.0000,\n",
      "         -0.4117,  0.4699],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.7841,  0.1516, -0.3514, -0.0000,\n",
      "          1.6582,  0.4151]])\n",
      "@ @Adup:Linear(in_features=10, out_features=0, bias=True) *id:1798851405392 >>\n",
      "    tensor([[ 2.2941, -4.4021, -0.0000,  1.2533,  0.0000,  2.9049,  0.0000, -0.0000,\n",
      "         -0.4117,  0.4699],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.7841,  0.1516, -0.3514, -0.0000,\n",
      "          1.6582,  0.4151]])\n",
      "  Net : Linear(in_features=10, out_features=0, bias=True)\n",
      "  $ (tensor([[ 2.2941, -4.4021, -0.0000,  1.2533,  0.0000,  2.9049,  0.0000, -0.0000,\n",
      "         -0.4117,  0.4699],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.7841,  0.1516, -0.3514, -0.0000,\n",
      "          1.6582,  0.4151]]),) >>\n",
      "  == tensor([], size=(2, 0), grad_fn=<AddmmBackward0>)\n",
      ")>> tensor([], size=(2, 0), grad_fn=<AddmmBackward0>)\n",
      "  0:fc : @Adup:Linear(in_features=10, out_features=0, bias=True) *id:1798851405392\n",
      "  $ (tensor([[ 2.2941, -4.4021, -0.0000,  1.2533,  0.0000,  2.9049,  0.0000, -0.0000,\n",
      "         -0.4117,  0.4699],\n",
      "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.7841,  0.1516, -0.3514, -0.0000,\n",
      "          1.6582,  0.4151]]),) >>\n",
      "  == tensor([], size=(2, 0), grad_fn=<AddmmBackward0>)\n",
      "@ @Adup:GELU(approximate='none') *id:1798851410624 >>\n",
      "    tensor([], size=(2, 0), grad_fn=<AddmmBackward0>)\n",
      "  Net : GELU(approximate='none')\n",
      "  $ (tensor([], size=(2, 0), grad_fn=<AddmmBackward0>),) >>\n",
      "  == tensor([], size=(2, 0), grad_fn=<GeluBackward0>)\n",
      ")>> tensor([], size=(2, 0), grad_fn=<GeluBackward0>)\n",
      "  1:act : @Adup:GELU(approximate='none') *id:1798851410624\n",
      "  $ (tensor([], size=(2, 0), grad_fn=<AddmmBackward0>),) >>\n",
      "  == tensor([], size=(2, 0), grad_fn=<GeluBackward0>)\n",
      ")>> tensor([], size=(2, 0), grad_fn=<GeluBackward0>)\n",
      "@ SEQ>Fn( >>\n",
      "    tensor([], size=(2, 0), grad_fn=<GeluBackward0>)\n",
      "@ @Adup:Linear(in_features=0, out_features=10, bias=True) *id:1798851409712 >>\n",
      "    tensor([], size=(2, 0), grad_fn=<GeluBackward0>)\n",
      "  Net : Linear(in_features=0, out_features=10, bias=True)\n",
      "  $ (tensor([], size=(2, 0), grad_fn=<GeluBackward0>),) >>\n",
      "  == tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<AddmmBackward0>)\n",
      ")>> tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<AddmmBackward0>)\n",
      "  0:fc : @Adup:Linear(in_features=0, out_features=10, bias=True) *id:1798851409712\n",
      "  $ (tensor([], size=(2, 0), grad_fn=<GeluBackward0>),) >>\n",
      "  == tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<AddmmBackward0>)\n",
      "@ @Adup:GELU(approximate='none') *id:1798851407024 >>\n",
      "    tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<AddmmBackward0>)\n",
      "  Net : GELU(approximate='none')\n",
      "  $ (tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<AddmmBackward0>),) >>\n",
      "  == tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<GeluBackward0>)\n",
      ")>> tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<GeluBackward0>)\n",
      "  1:act : @Adup:GELU(approximate='none') *id:1798851407024\n",
      "  $ (tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<AddmmBackward0>),) >>\n",
      "  == tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<GeluBackward0>)\n",
      ")>> tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<GeluBackward0>)\n",
      ")>> tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<GeluBackward0>)\n",
      "@ @Adup:Linear(in_features=10, out_features=1, bias=True) *id:1798851410384 >>\n",
      "    tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<GeluBackward0>)\n",
      "  Net : Linear(in_features=10, out_features=1, bias=True)\n",
      "  $ (tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<GeluBackward0>),) >>\n",
      "  == tensor([[-0.1061],\n",
      "        [-0.1061]], grad_fn=<AddmmBackward0>)\n",
      ")>> tensor([[-0.1061],\n",
      "        [-0.1061]], grad_fn=<AddmmBackward0>)\n",
      "  2:fc : @Adup:Linear(in_features=10, out_features=1, bias=True) *id:1798851410384\n",
      "  $ (tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<GeluBackward0>),) >>\n",
      "  == tensor([[-0.1061],\n",
      "        [-0.1061]], grad_fn=<AddmmBackward0>)\n",
      ")>> tensor([[-0.1061],\n",
      "        [-0.1061]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1061],\n",
       "        [-0.1061]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.p(torch.randn(2,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
