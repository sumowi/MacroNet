{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/myf/Skorch/src/MoNet\n"
     ]
    }
   ],
   "source": [
    "%cd \"~/Skorch/src/MoNet/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "普通方法\n",
    "\n",
    "- 书写计算过程可读性差，括号太多\n",
    "  该代码定义了一个具有三个全连接层和 PReLU 激活函数的神经网络。然后，它将该网络应用于输入张量并计算三个不同的输出张量。\n",
    "\n",
    "步骤如下：\n",
    "\n",
    "1. 导入 torch 库。\n",
    "2. 使用三个全连接层定义神经网络架构：Fc1、Fc2 和 Fc3。\n",
    "3. 使用 \\_parameters.values() 方法提取每个层的权重和偏置参数，并将它们分配给变量 W1、b1、W2、b2、W3 和 b3。\n",
    "4. 创建 PReLU 激活函数的实例并将其分配给变量 f。\n",
    "5. 创建一个具有值 [1、2、3] 的输入张量 x 并将其转换为 float 数据类型。\n",
    "6. 使用矩阵乘法和激活函数将网络应用于输入张量，计算输出张量 y1。\n",
    "7. 直接使用全连接层和激活函数将网络应用于输入张量，计算输出张量 y2。\n",
    "8. 使用 Sequential 容器将层和激活函数组合起来，将网络应用于输入张量，计算输出张量 y3。\n",
    "9. 打印 y1、y2 和 y3 的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1832], grad_fn=<AddBackward0>),\n",
       " tensor([0.1832], grad_fn=<ViewBackward0>),\n",
       " tensor([0.1832], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# 定义一个神经网络 Net = 2(FcAct)Fc\n",
    "W1, b1 = (Fc1 := torch.nn.Linear(3, 5))._parameters.values()\n",
    "W2, b2 = (Fc2 := torch.nn.Linear(5, 5))._parameters.values()\n",
    "W3, b3 = (Fc3 := torch.nn.Linear(5, 1))._parameters.values()\n",
    "f = torch.nn.PReLU()\n",
    "\n",
    "x = torch.tensor([1, 2, 3]).float()\n",
    "y1 = f(f(x.matmul(W1.T) + b1).matmul(W2.T) + b2).matmul(W3.T) + b3\n",
    "y2 = Fc3(f(Fc2(f(Fc1(x)))))\n",
    "y3 = torch.nn.Sequential(Fc1, f, Fc2, f, Fc3)(x)\n",
    "y1, y2, y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MoNet 方法\n",
    "\n",
    "- 书写计算过程可读性好，括号少, 直接用乘法表示顺序计算关系, 同时兼容普通方式调用方法\n",
    "- 提供 Layer,SeqLayer,Cell,SeqCell,Mix 接口快速构建网络\n",
    "\n",
    "代码的步骤解释如下：\n",
    "\n",
    "1. 导入 monet 库。\n",
    "2. 使用 Layer 函数定义了一个具有 3 个输入和 5 个输出的全连接层 Fc1。\n",
    "3. 使用 Layer 函数定义了一个具有 5 个输入和 5 个输出的全连接层 Fc2。\n",
    "4. 使用 Layer 函数定义了一个具有 5 个输入和 1 个输出的全连接层 Fc3。\n",
    "5. 使用 Layer 函数定义了一个激活函数层 f。\n",
    "6. 使用 torch.tensor 函数创建了一个包含元素[1,2,3]的张量 x，并将其转换为浮点型。\n",
    "7. 将输入张量 x 通过层序列（Fc1\\*f\\*Fc2\\*f\\*Fc3）进行前向传播计算，得到输出张量 y。\n",
    "8. 输出张量 y。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0311], grad_fn=<ViewBackward0>),\n",
       " tensor([-0.0311], grad_fn=<ViewBackward0>),\n",
       " tensor([-0.5666], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monet import *\n",
    "\n",
    "Fc1 = Layer(3, 5)\n",
    "Fc2 = Layer(5, 5)\n",
    "Fc3 = Layer(5, 1)\n",
    "f = Layer(net='act')\n",
    "\n",
    "x = torch.tensor([1, 2, 3]).float()\n",
    "y1 = (Fc1 * f * Fc2 * f * Fc3)(x)  # 看起来最直观\n",
    "y2 = torch.nn.Sequential(Fc1, f, Fc2, f, Fc3)(x)\n",
    "y3 = Mix(3, [[5, 5], 1], [['fc', 'act'], 'fc'])(x)  #一行代码即可构建完整神经网络\n",
    "\n",
    "y1, y2, y3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自适应调整输入维度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ>MoNet(\n",
      "  (Net): Sequential(\n",
      "    (0): SEQ>SeqLayer(\n",
      "      (Net): Sequential(\n",
      "        (fc_1_0): Linear(in_features=0, out_features=64, bias=True)\n",
      "        (fc_1_1): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): SEQ>Layer(\n",
      "      (Net): Linear(in_features=0, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0964],\n",
       "         [ 0.1891]], grad_fn=<AddmmBackward0>),\n",
       " SEQ>MoNet(\n",
       "   (Net): Sequential(\n",
       "     (0): SEQ>SeqLayer(\n",
       "       (Net): Sequential(\n",
       "         (fc_1_0): Linear(in_features=5, out_features=64, bias=True)\n",
       "         (fc_1_1): Linear(in_features=64, out_features=64, bias=True)\n",
       "       )\n",
       "     )\n",
       "     (1): SEQ>Layer(\n",
       "       (Net): Linear(in_features=64, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把需要自适应的输入维度设为0即可，代入数据后会自行调整\n",
    "from monet import *\n",
    "\n",
    "F = SeqLayer(0) * Layer(0, 1)\n",
    "print(F)\n",
    "F(torch.randn(2, 5)), F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0631],\n",
       "        [-0.0631]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monet import *\n",
    "\n",
    "F = Mix(0) * SeqCell(0) * Cell(0) * SeqLayer(0) * Layer(0, 1)\n",
    "F(torch.randn(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与nn.Module一起使用, 减少代码量的同时灵活处理信息流动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (cnn_img): Sequential(\n",
      "    (0:input): SeqCell(\n",
      "      (Net): Sequential(\n",
      "        (0): Cell(\n",
      "          (Net): Sequential(\n",
      "            (0:cv): Conv2d(0, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (1:bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2:mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (3:act): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "        (1): Cell(\n",
      "          (Net): Sequential(\n",
      "            (0:cv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "            (1:bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2:mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (3:act): PReLU(num_parameters=1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1:output): Layer(\n",
      "      (Net): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "  )\n",
      "  (txt_fc): Sequential(\n",
      "    (0:input): SeqCell(\n",
      "      (Net): Sequential(\n",
      "        (0): Cell(\n",
      "          (Net): Sequential(\n",
      "            (0:fc): Linear(in_features=0, out_features=32, bias=True)\n",
      "            (1:bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2:act): PReLU(num_parameters=1)\n",
      "            (3:dp): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Cell(\n",
      "          (Net): Sequential(\n",
      "            (0:fc): Linear(in_features=32, out_features=64, bias=True)\n",
      "            (1:bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2:act): PReLU(num_parameters=1)\n",
      "            (3:dp): Dropout(p=0.5, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (out_mlp): Sequential(\n",
      "    (0:input): Layer(\n",
      "      (Net): Linear(in_features=0, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.7318],\n",
       "         [-0.5621]], grad_fn=<AddmmBackward0>),\n",
       " Net(\n",
       "   (cnn_img): Sequential(\n",
       "     (0:input): SeqCell(\n",
       "       (Net): Sequential(\n",
       "         (0): Cell(\n",
       "           (Net): Sequential(\n",
       "             (0:cv): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "             (1:bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "             (2:mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "             (3:act): PReLU(num_parameters=1)\n",
       "           )\n",
       "         )\n",
       "         (1): Cell(\n",
       "           (Net): Sequential(\n",
       "             (0:cv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "             (1:bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "             (2:mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "             (3:act): PReLU(num_parameters=1)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1:output): Layer(\n",
       "       (Net): Flatten(start_dim=1, end_dim=-1)\n",
       "     )\n",
       "   )\n",
       "   (txt_fc): Sequential(\n",
       "     (0:input): SeqCell(\n",
       "       (Net): Sequential(\n",
       "         (0): Cell(\n",
       "           (Net): Sequential(\n",
       "             (0:fc): Linear(in_features=10, out_features=32, bias=True)\n",
       "             (1:bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "             (2:act): PReLU(num_parameters=1)\n",
       "             (3:dp): Dropout(p=0.5, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (1): Cell(\n",
       "           (Net): Sequential(\n",
       "             (0:fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "             (1:bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "             (2:act): PReLU(num_parameters=1)\n",
       "             (3:dp): Dropout(p=0.5, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (out_mlp): Sequential(\n",
       "     (0:input): Layer(\n",
       "       (Net): Linear(in_features=6336, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monet import *\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.cnn_img = Mix(0, [[16, 32], 1],\n",
    "                           [['cv2', 'bn2', 'mp2', 'act'], 'fl']).Net\n",
    "        self.txt_fc = Mix(0, [[32, 64]], [['fc', 'bn', 'act', 'dp']]).Net\n",
    "        self.out_mlp = Mix(0, [1], ['fc']).Net\n",
    "\n",
    "    def forward(self, img, x):\n",
    "        y1 = self.cnn_img(img)\n",
    "        y2 = self.txt_fc(x)\n",
    "        y3 = torch.cat([y1, y2], dim=1)\n",
    "        y = self.out_mlp(y3)\n",
    "        return y\n",
    "\n",
    "\n",
    "F = Net()\n",
    "print(F)\n",
    "F(torch.randn(2, 1, 64, 64), torch.randn(2, 10)), F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入相同时，可直接用加法将多层神经网络输出进行合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoNet(\n",
      "  (Net): Sequential(\n",
      "    (0): MoNet(\n",
      "      (Net): ModuleList(\n",
      "        (0): Mix(\n",
      "          (Net): Sequential(\n",
      "            (0:input): SeqCell(\n",
      "              (Net): Sequential(\n",
      "                (0): Cell(\n",
      "                  (Net): Sequential(\n",
      "                    (0:cv): Conv2d(0, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "                    (1:bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                    (2:mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "                    (3:act): PReLU(num_parameters=1)\n",
      "                  )\n",
      "                )\n",
      "                (1): Cell(\n",
      "                  (Net): Sequential(\n",
      "                    (0:cv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "                    (1:bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                    (2:mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "                    (3:act): PReLU(num_parameters=1)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1:output): Layer(\n",
      "              (Net): Flatten(start_dim=1, end_dim=-1)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Mix(\n",
      "          (Net): Sequential(\n",
      "            (0:input): SeqCell(\n",
      "              (Net): Sequential(\n",
      "                (0): Cell(\n",
      "                  (Net): Sequential(\n",
      "                    (0:fc): Linear(in_features=0, out_features=32, bias=True)\n",
      "                    (1:act): PReLU(num_parameters=1)\n",
      "                    (2:dp): Dropout(p=0.5, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): Cell(\n",
      "                  (Net): Sequential(\n",
      "                    (0:fc): Linear(in_features=32, out_features=64, bias=True)\n",
      "                    (1:act): PReLU(num_parameters=1)\n",
      "                    (2:dp): Dropout(p=0.5, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1:output): Layer(\n",
      "              (Net): Flatten(start_dim=1, end_dim=-1)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Layer(\n",
      "      (Net): Linear(in_features=0, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0351],\n",
       "         [ 0.4670]], grad_fn=<AddmmBackward0>),\n",
       " MoNet(\n",
       "   (Net): Sequential(\n",
       "     (0): MoNet(\n",
       "       (Net): ModuleList(\n",
       "         (0): Mix(\n",
       "           (Net): Sequential(\n",
       "             (0:input): SeqCell(\n",
       "               (Net): Sequential(\n",
       "                 (0): Cell(\n",
       "                   (Net): Sequential(\n",
       "                     (0:cv): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "                     (1:bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (2:mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "                     (3:act): PReLU(num_parameters=1)\n",
       "                   )\n",
       "                 )\n",
       "                 (1): Cell(\n",
       "                   (Net): Sequential(\n",
       "                     (0:cv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "                     (1:bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                     (2:mp): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "                     (3:act): PReLU(num_parameters=1)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (1:output): Layer(\n",
       "               (Net): Flatten(start_dim=1, end_dim=-1)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (1): Mix(\n",
       "           (Net): Sequential(\n",
       "             (0:input): SeqCell(\n",
       "               (Net): Sequential(\n",
       "                 (0): Cell(\n",
       "                   (Net): Sequential(\n",
       "                     (0:fc): Linear(in_features=64, out_features=32, bias=True)\n",
       "                     (1:act): PReLU(num_parameters=1)\n",
       "                     (2:dp): Dropout(p=0.5, inplace=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (1): Cell(\n",
       "                   (Net): Sequential(\n",
       "                     (0:fc): Linear(in_features=32, out_features=64, bias=True)\n",
       "                     (1:act): PReLU(num_parameters=1)\n",
       "                     (2:dp): Dropout(p=0.5, inplace=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (1:output): Layer(\n",
       "               (Net): Flatten(start_dim=1, end_dim=-1)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1): Layer(\n",
       "       (Net): Linear(in_features=10368, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monet import *\n",
    "F = (Mix(0, [[16, 32], 1],[['cv2', 'bn2', 'mp2', 'act'], 'fl'])+\n",
    "    Mix(0, [[32, 64], 1], [['fc', 'act', 'dp'],'fl']))*Layer(0)\n",
    "print(F)\n",
    "F(torch.randn(2, 1, 64, 64)), F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "乘法计算满足结合律与交换律"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monet import *\n",
    "\n",
    "Fc = Cell(1, 5, ['fc', 'act'])\n",
    "Hid1 = Cell(5, 1, ['fc', 'act'])\n",
    "Hid2 = Cell(5, 1, ['fc', 'act'])\n",
    "Out = Layer(2, 1, 'fc')\n",
    "\n",
    "x = torch.tensor([[0.0]])\n",
    "F1 = (Fc * Hid1 + Fc * Hid2) * Out\n",
    "F2 = Fc * (Hid1 + Hid2) * Out\n",
    "F1(x) == F2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首字母大写返回一个组件，首字母小写构建方法会返回所有子成员"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoNet(\n",
       "  (Net): Sequential(\n",
       "    (0): Cell(\n",
       "      (Net): Sequential(\n",
       "        (0:fc): Linear(in_features=10, out_features=1, bias=True)\n",
       "        (1:bn): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2:act): PReLU(num_parameters=1)\n",
       "        (3:dp): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Linear(in_features=10, out_features=1, bias=True)\n",
       "    (2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): PReLU(num_parameters=1)\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monet import *\n",
    "\n",
    "Cell() * cell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* 与 + 使用浅拷贝，\\*\\*与&使用深拷贝，\\*与\\*\\*后面可接数字，表示拷贝次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23275250179840, 23275265044208)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = layer()\n",
    "A2 = layer()\n",
    "id(A), id(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23275250179840, 23275250179840, 23275250179840)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id((A * 1)), id((A * 2).Net[0]), id((A * 2).Net[1])  # 浅拷贝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23275250179840, 23275250179840, 23275265044208)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = A * A * A2  # 浅拷贝\n",
    "id(B.Net[0]), id(B.Net[1]), id(B.Net[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23275240731600, 23275250182096, 23275267990736)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id((A**1)), id((A**2).Net[0]), id((A**2).Net[1])  # 深拷贝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23275247281840, 23275247279632, 23275247225920)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = (A**A)**3  # 深拷贝\n",
    "id(B.Net[0].Net[0]), id(B.Net[1].Net[0]), id(B.Net[2].Net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23275267984112, 23275250179840, 23275250179840)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id((A + A)), id((A + A).Net[0]), id((A + A).Net[1])  # 浅拷贝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23275240735888, 23275247280976, 23275247224192)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id((A & A)), id((A & A).Net[0]), id((A & A).Net[1])  # 深拷贝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自适应调整输入形状\n",
    "\n",
    "- 设置输入维度为 0，用 set_i()或在首次 forward 时自适应调整顶层输入形状\n",
    "- 输入维度不为 0，用 set_i()可强制更新顶层输入形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3071]], grad_fn=<AddmmBackward0>),\n",
       " MoNet(\n",
       "   (Net): Sequential(\n",
       "     (0): Cell(\n",
       "       (Net): Sequential(\n",
       "         (0:fc): Linear(in_features=1, out_features=10, bias=True)\n",
       "         (1:act): PReLU(num_parameters=1)\n",
       "         (2:fc): Linear(in_features=10, out_features=10, bias=True)\n",
       "         (3:act): PReLU(num_parameters=1)\n",
       "         (4:fc): Linear(in_features=10, out_features=10, bias=True)\n",
       "         (5:act): PReLU(num_parameters=1)\n",
       "       )\n",
       "     )\n",
       "     (1): MoNet(\n",
       "       (Net): Linear(in_features=10, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monet import *\n",
    "\n",
    "F = Cell(1, 10, ['fc', 'act', 'fc', 'act', 'fc', 'act']) * layer(0, 1)\n",
    "F(torch.tensor([[0.0]])), F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2324]], grad_fn=<AddmmBackward0>),\n",
       " MoNet(\n",
       "   (Net): Sequential(\n",
       "     (0): Cell(\n",
       "       (Net): Sequential(\n",
       "         (0:fc): Linear(in_features=2, out_features=10, bias=True)\n",
       "         (1:act): PReLU(num_parameters=1)\n",
       "         (2:fc): Linear(in_features=10, out_features=10, bias=True)\n",
       "         (3:act): PReLU(num_parameters=1)\n",
       "         (4:fc): Linear(in_features=10, out_features=10, bias=True)\n",
       "         (5:act): PReLU(num_parameters=1)\n",
       "       )\n",
       "     )\n",
       "     (1): MoNet(\n",
       "       (Net): Linear(in_features=10, out_features=1, bias=True)\n",
       "     )\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.set_i(2)(torch.tensor([[0.0, 1.0]])), F # 强制更新"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.venv]",
   "language": "python",
   "name": "conda-env-.venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
